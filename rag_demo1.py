# -*- coding: utf-8 -*-
"""RAG_demo1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FT5irvNZRdrC8C_glbDWtA10T5OB6Soe

Create a virtual environment:
"""

!python -m venv rag_env
!source rag_env/bin/activate

pip install pandas numpy streamlit sentence-transformers langchain milvus pymilvus openai PyPDF2 SpeechRecognition ffmpeg

"""Create a RAG pipeline

Insstalling Dependencies
"""

# Install necessary libraries
!pip install sentence-transformers
!pip install transformers
!pip install pymilvus
!pip install torch
!pip install docker
!pip install streamlit

"""Uplaoding the relavant documents

"""

from google.colab import drive
drive.mount('/content/drive')

"""data cleaning for text file"""

import nltk
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Download necessary NLTK data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('punkt_tab') # Download the missing 'punkt_tab' data package

# Load stop words
stop_words = set(stopwords.words('english'))

# Function to clean the text: Remove punctuation, stop words, and lowercasing
def clean_text(text):
    # Convert to lowercase
    text = text.lower()

    # Remove punctuation
    text = ''.join([char for char in text if char not in string.punctuation])

    # Tokenize the text
    words = word_tokenize(text)

    # Remove stop words
    cleaned_words = [word for word in words if word not in stop_words]

    # Rejoin words to form cleaned text
    cleaned_text = ' '.join(cleaned_words)

    return cleaned_text

# Path to the text file
text_file_path = '/content/drive/MyDrive/MES_text.txt'

# Read the text file
with open(text_file_path, 'r') as file:
    text_data = file.read()

# Clean the text
cleaned_text = clean_text(text_data)

# Print cleaned text
print("Cleaned Text from txt file:\n")
print(cleaned_text[:500])  # Print the first 500 characters

"""Tokenize the Text (Optional but Recommended for Embeddings)
Tokenizing helps break the cleaned text into smaller chunks (words or sub-words), which are often required for embedding models.

"""

# Tokenize the cleaned text
tokens = word_tokenize(cleaned_text)

# Print the first few tokens
print("Tokens from the cleaned text:\n")
print(tokens[:50])  # Print first 50 tokens

"""Prepare for Embedding Generation
Once the text is cleaned and tokenized, itâ€™s ready for generating embeddings using a model like Sentence-Transformer. We can now prepare this data for that step
"""

# Prepare the cleaned and tokenized text for embeddings
# You can save these tokens or cleaned text to a variable, or store them for future use
processed_data = cleaned_text  # or tokens if you need them in tokenized form

# Optionally, you can store it in a list or other data structure if needed
documents = [processed_data]  # This list will contain the processed text

# Print the final processed text (cleaned and tokenized)
print("Final Processed Data (cleaned text):\n")
print(processed_data[:500])  # Print first 500 characters

""" Save Processed Data for Future Use (Optional)
You may want to save this cleaned and processed data for future steps (like embedding generation or storing in the vector database).
"""

# Save cleaned text to a new file (if required)
output_file_path = '/content/drive/MyDrive/MES_Files/processed_text.txt'
with open(output_file_path, 'w') as output_file:
    output_file.write(processed_data)

print(f"Processed text has been saved to {output_file_path}")

"""**PDF FORMAT**"""

!pip install pymupdf
!pip install nltk

import fitz  # PyMuPDF
import nltk
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Download necessary NLTK data
nltk.download('punkt')
nltk.download('stopwords')

# Load stop words
stop_words = set(stopwords.words('english'))

"""Extract Text from PDF"""

# Function to extract text from PDF
def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    text = ""
    for page in doc:
        text += page.get_text()
    return text

# Path to the PDF file
pdf_file_path = '/content/drive/MyDrive/MES_gmpua.pdf'

# Extract text from the PDF
pdf_text = extract_text_from_pdf(pdf_file_path)

# Print the first 500 characters of the extracted text
print("Extracted Text from PDF file:\n")
print(pdf_text[:500])  # Print first 500 characters to preview

"""Clean the Text

"""

# Function to clean the text: Remove punctuation, stop words, and lowercasing
def clean_text(text):
    # Convert to lowercase
    text = text.lower()

    # Remove punctuation
    text = ''.join([char for char in text if char not in string.punctuation])

    # Tokenize the text
    words = word_tokenize(text)

    # Remove stop words
    cleaned_words = [word for word in words if word not in stop_words]

    # Rejoin words to form cleaned text
    cleaned_text = ' '.join(cleaned_words)

    return cleaned_text

# Clean the extracted PDF text
cleaned_pdf_text = clean_text(pdf_text)

# Print cleaned text
print("Cleaned Text from PDF file:\n")
print(cleaned_pdf_text[:500])  # Print first 500 characters of cleaned text

"""Tokenize the Cleaned Text"""

# Tokenize the cleaned text
tokens = word_tokenize(cleaned_pdf_text)

# Print the first few tokens
print("Tokens from the cleaned PDF text:\n")
print(tokens[:50])  # Print first 50 tokens

"""Prepare for Further Use (Embedding Generation)
Finally, you can store the cleaned and tokenized text, or the raw cleaned text, for future use like embedding generation.
"""

# Store the processed data (cleaned text) for future use
processed_pdf_data = cleaned_pdf_text  # Or tokens if needed

# Print the processed data
print("Processed Data (Cleaned Text):\n")
print(processed_pdf_data[:500])  # Print first 500 characters of cleaned data

"""Optionally Save the Cleaned PDF Text
You might want to save the cleaned PDF text to a file for later use.

python
Copy code

"""

import os

# Save cleaned text to a new file (optional)
output_pdf_path = '/content/drive/MyDrive/MES_Files/processed_pdf_text.txt'

# Create the directory if it doesn't exist
os.makedirs(os.path.dirname(output_pdf_path), exist_ok=True)

with open(output_pdf_path, 'w') as output_file:
    output_file.write(processed_pdf_data)

print(f"Processed PDF text has been saved to {output_pdf_path}")

"""**AUDIO FORMAT**"""

!apt-get install ffmpeg
!pip install SpeechRecognition pydub
!pip install nltk

"""Extract Audio from Video
We first extract the audio from the video file since transcription tools work on audio. Use FFmpeg for this.
"""

import os

# Paths
video_file_path = '/content/drive/MyDrive/Manufacturing Execution System - What is it.mp4'  # Input video file
audio_file_path = '/content/drive/MyDrive/MES_Files/audio_from_video.wav'  # Extracted audio file

# Extract audio from video
os.system(f"ffmpeg -i {video_file_path} -q:a 0 -map a {audio_file_path}")
print(f"Audio extracted and saved at {audio_file_path}")

"""Enhance Audio Quality
Enhance the audio to improve transcription accuracy. This involves noise reduction, normalization, and frequency filtering.

Code for Noise Reduction:
"""

import os
import librosa
import noisereduce as nr
import soundfile as sf
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')  # Mounts Google Drive to the Colab environment

# Paths
video_file_path = '/content/drive/MyDrive/Manufacturing Execution System - What is it.mp4'  # Input video file
audio_file_path = '/content/drive/MyDrive/MES_Files/audio_from_video.wav'  # Extracted audio file
enhanced_audio_path = '/content/drive/MyDrive/MES_Files/enhanced_audio.wav'

# Ensure the directory exists
os.makedirs(os.path.dirname(audio_file_path), exist_ok=True)  # Creates the necessary directories for the audio file

# Extract audio from video using ffmpeg
print("Extracting audio from video...")
return_code = os.system(f"ffmpeg -i \"{video_file_path}\" -q:a 0 -map a \"{audio_file_path}\"")
print(f"ffmpeg return code: {return_code}")
if return_code != 0:
    raise Exception(f"ffmpeg failed with return code {return_code}. Check video_file_path and permissions.")
print(f"Audio extracted and saved at {audio_file_path}")

# Add check for file existence
if not os.path.exists(audio_file_path):
    raise FileNotFoundError(f"Audio file not found at: {audio_file_path}")

# Noise reduction function
def reduce_noise(input_path, output_path):
    print(f"Reducing noise in the audio: {input_path}...")
    # Load the audio
    try:
        y, sr = librosa.load(input_path)
    except Exception as e:
        raise Exception(f"Error loading audio file: {e}")

    # Extract noise profile (assume first 0.5 seconds is noise)
    noise_clip = y[:int(sr * 0.5)]

    # Reduce noise
    try:
        reduced_audio = nr.reduce_noise(y=y, sr=sr, y_noise=noise_clip)
    except Exception as e:
        raise Exception(f"Error during noise reduction: {e}")

    # Save enhanced audio
    try:
        sf.write(output_path, reduced_audio, sr)
        print(f"Noise-reduced audio saved at {output_path}")
    except Exception as e:
        raise Exception(f"Error saving enhanced audio: {e}")

# Enhance audio quality
reduce_noise(audio_file_path, enhanced_audio_path)

"""(Optional) Split Audio into Smaller Chunks
If the audio is long, split it into smaller chunks for better transcription results.
"""

from pydub import AudioSegment

# Directory for audio chunks
chunk_dir = '/content/drive/MyDrive/MES_Files/audio_chunks/'
os.makedirs(chunk_dir, exist_ok=True)

# Load the enhanced audio
audio = AudioSegment.from_wav(enhanced_audio_path)

# Split into 30-second chunks
chunk_length_ms = 30 * 1000  # 30 seconds in milliseconds
chunks = [audio[i:i + chunk_length_ms] for i in range(0, len(audio), chunk_length_ms)]

# Save each chunk
chunk_paths = []
for idx, chunk in enumerate(chunks):
    chunk_path = os.path.join(chunk_dir, f'chunk_{idx}.wav')
    chunk.export(chunk_path, format="wav")
    chunk_paths.append(chunk_path)
    print(f"Saved chunk {idx} to {chunk_path}")

print(f"Audio has been split into {len(chunk_paths)} chunks.")

"""Transcribe Audio to Text
Transcribe the enhanced audio or its chunks using Whisper for robust transcription.

Install Whisper
"""

!pip install openai-whisper ffmpeg

"""Transcription code"""

import whisper

# Load the Whisper model
model = whisper.load_model("base")

# Transcribe the enhanced audio
result = model.transcribe(enhanced_audio_path)
transcribed_text = result["text"]
print("Transcribed Text:\n", transcribed_text)

"""For audio chunks"""

# Transcribe each chunk
transcribed_chunks = []
for chunk_path in chunk_paths:
    result = model.transcribe(chunk_path)
    transcribed_chunks.append(result["text"])

# Combine transcribed text
final_transcribed_text = " ".join(transcribed_chunks)
print("Final Transcribed Text:\n", final_transcribed_text)

"""Clean the Transcribed Text
Remove unnecessary noise like punctuation, stopwords, and convert to lowercase.
"""

import nltk
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')

# Stopwords and cleaning function
stop_words = set(stopwords.words('english'))

def clean_text(text):
    text = text.lower()  # Convert to lowercase
    text = ''.join([char for char in text if char not in string.punctuation])  # Remove punctuation
    words = word_tokenize(text)  # Tokenize
    words = [word for word in words if word not in stop_words]  # Remove stopwords
    return ' '.join(words)

# Clean the transcribed text
cleaned_text = clean_text(transcribed_text if len(chunk_paths) == 0 else final_transcribed_text)
print("Cleaned Transcribed Text:\n", cleaned_text)

"""Saving the Transcribed Text
If you want to save the transcribed or cleaned text into a file for later use, use the following code:

Save the Raw Transcription:
"""

raw_text_path = '/content/drive/MyDrive/MES_Files/transcribed_text.txt'
with open(raw_text_path, 'w') as f:
    f.write(transcribed_text if len(chunk_paths) == 0 else final_transcribed_text)
print(f"Transcribed text saved at {raw_text_path}")

"""Save the Cleaned Transcription:
python
Copy code

"""

cleaned_text_path = '/content/drive/MyDrive/MES_Files/cleaned_transcribed_text.txt'
with open(cleaned_text_path, 'w') as f:
    f.write(cleaned_text)
print(f"Cleaned transcribed text saved at {cleaned_text_path}")

"""MAKING EMBEDDINGS OUT OF THEM"""

!pip install sentence-transformers

from sentence_transformers import SentenceTransformer
import numpy as np

# Load the SentenceTransformer model
model = SentenceTransformer('all-MiniLM-L6-v2')  # Efficient model for embeddings

# Define paths for your cleaned text files
# Define paths for your cleaned text files (check and update the paths if necessary)
cleaned_text_files = {
    'txt': '/content/drive/MyDrive/MES_Files/processed_text.txt',  # Cleaned text from TXT file
    'pdf': '/content/drive/MyDrive/MES_Files/processed_pdf_text.txt', # Cleaned text from PDF
    'audio': '/content/drive/MyDrive/MES_Files/cleaned_transcribed_text.txt'  # Cleaned text from Transcription
}



# Dictionary to hold the embeddings
embeddings_dict = {}

# Load and encode the text files
for key, file_path in cleaned_text_files.items():
    try:
        with open(file_path, 'r') as f:
            cleaned_text = f.read()

        # Generate embeddings for each cleaned text
        embeddings = model.encode([cleaned_text])

        # Store embeddings in the dictionary
        embeddings_dict[key] = embeddings

        # Save embeddings for each format separately
        embeddings_file_path = f"/content/drive/MyDrive/MES_Files/embeddings_{key}.npy"
        np.save(embeddings_file_path, embeddings)
        print(f"Embeddings for {key} saved at {embeddings_file_path}")

    except FileNotFoundError:
        print(f"Error: File not found at {file_path}. Please check the file path and ensure the file exists.")

# Optionally: Print out the shape of embeddings to confirm
for key, emb in embeddings_dict.items():
    print(f"Embeddings for {key} shape: {np.array(emb).shape}")

""" FAISS VECTOR DATABASE"""

!pip install faiss-cpu  # For CPU-only version
# or
!pip install faiss-gpu  # For GPU-accelerated version (if you have a compatible GPU)

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Your embeddings will likely be located at '/content/drive/My Drive/...'
# Example path to your files on Google Drive
path_to_txt_embeddings = '/content/drive/MyDrive/MES_Files/embeddings_txt.npy'
path_to_pdf_embeddings = '/content/drive/MyDrive/MES_Files/embeddings_pdf.npy'
path_to_audio_embeddings = '/content/drive/MyDrive/MES_Files/embeddings_audio.npy'

import numpy as np

# Load the embeddings from Google Drive
embeddings_txt = np.load(path_to_txt_embeddings)
embeddings_pdf = np.load(path_to_pdf_embeddings)
embeddings_audio = np.load(path_to_audio_embeddings)

# Check their shapes to make sure they are compatible
print("Text Embedding Shape:", embeddings_txt.shape)
print("PDF Embedding Shape:", embeddings_pdf.shape)
print("Audio Embedding Shape:", embeddings_audio.shape)

import faiss

# Check the dimensionality of the embeddings (assuming all have the same dimension)
embedding_dimension = embeddings_txt.shape[1]  # Example: 384

# Create the FAISS index (using IndexFlatIP for cosine similarity)
index = faiss.IndexFlatIP(embedding_dimension)

# Combine all the embeddings into a single numpy array (stacking vertically)
all_embeddings = np.vstack((embeddings_txt, embeddings_pdf, embeddings_audio))

# Add all the embeddings to the FAISS index
index.add(all_embeddings)

# Verify the number of vectors in the index
print("Number of vectors in the FAISS index:", index.ntotal)

# Example query: create a random vector to simulate a query (this should ideally be an actual query embedding)
query_embedding = np.random.random((1, embedding_dimension)).astype('float32')

# Retrieve the top 5 nearest neighbors
k = 5
distances, indices = index.search(query_embedding, k)

print("Top 5 nearest neighbors' indices:", indices)
print("Top 5 nearest neighbors' distances:", distances)

# Save the FAISS index to a file
faiss.write_index(index, '/content/drive/My Drive/faiss_index.index')

# You can later read the index back from the saved file
# index = faiss.read_index('/content/drive/My Drive/faiss_index.index')

# Example query: Create a random query vector for testing (in real scenarios, this will be your query embedding)
query_embedding = np.random.random((1, embedding_dimension)).astype('float32')

# Retrieve the top 5 nearest neighbors
k = 5
distances, indices = index.search(query_embedding, k)

# Display the results
print("Top 5 nearest neighbors' indices:", indices)
print("Top 5 nearest neighbors' distances:", distances)

"""**Prepare Query Embedding**

Step 1: Load the necessary libraries and the embeddings
python
Copy code
"""

!pip install SpeechRecognition

!pip install pydub
!pip install SpeechRecognition
!apt-get install ffmpeg  # Install ffmpeg if you are using MP3 files

import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
import pickle
import os

# Step 1: Load the Sentence Transformer model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Step 2: Load your embeddings from Google Drive (make sure they are in numpy format)
embeddings_txt = np.load('/content/drive/MyDrive/MES_Files/embeddings_txt.npy')
embeddings_pdf = np.load('/content/drive/MyDrive/MES_Files/embeddings_pdf.npy')
embeddings_audio = np.load('/content/drive/MyDrive/MES_Files/embeddings_audio.npy')

# Step 3: Load the original documents (actual text files or other formats)
documents_txt = ["MES_text"

]  # Example for text documents

documents_pdf = ["MES_gmpua"

]  # Example for PDF documents

documents_audio = ["Manufacturing Execution System - What is it"

]  # Example for audio documents

# Step 4: Combine all embeddings and documents
all_embeddings = np.concatenate([embeddings_txt, embeddings_pdf, embeddings_audio], axis=0)
all_documents = documents_txt + documents_pdf + documents_audio  # Combine all document contents

# Step 5: Create or load FAISS index
dim = all_embeddings.shape[1]  # Dimensionality of the embeddings
index = faiss.IndexFlatL2(dim)  # Using L2 distance metric
index.add(all_embeddings)  # Add embeddings to the index

# Step 6: Map FAISS indices to documents
document_map = {i: all_documents[i] for i in range(len(all_documents))}

# Step 7: Save the document_map for later retrieval (in case you want to load it later)
with open('/content/drive/MyDrive/document_map.pkl', 'wb') as f:
    pickle.dump(document_map, f)

# Step 8: Function to retrieve corresponding document based on FAISS search result
def retrieve_documents_from_faiss(query_embedding, index, document_map, k=5):
    # Perform the search
    _, indices = index.search(query_embedding, k)  # Get top k results

    # Retrieve the documents based on the FAISS indices, ignoring -1 indices
    retrieved_documents = [document_map[i] for i in indices[0] if i != -1]
    return retrieved_documents
# step 9
file_path = "/content/drive/MyDrive/MES_Files/cleaned_transcribed_text.txt"  # Update with the correct path
if not os.path.exists(file_path):
    raise FileNotFoundError(f"The file '{file_path}' does not exist. "
                            f"Please make sure the file is present or provide the correct path.")

with open(file_path, "r") as file:
    transcribed_text = file.read()

# Step 10: Generate the embedding for the transcribed text
query_embedding = model.encode([transcribed_text])

# Step 11: Retrieve relevant documents based on the query embedding
retrieved_docs = retrieve_documents_from_faiss(query_embedding, index, document_map)

# Step 12: Print the retrieved documents
for doc in retrieved_docs:
    print(doc)

"""Step 2: Write a function to process the query and search the database"""

def search_query(query, index, model, top_k=5):
    # Convert query to embedding
    query_embedding = model.encode([query])

    # Perform the search in the FAISS index
    distances, indices = index.search(query_embedding, top_k)

    return distances, indices

"""Step 3: Retrieve the most relevant results from FAISS"""

# Example query related to MES
query = "What is the process of material management in MES?"

# Search the query in the FAISS index
distances, indices = search_query(query, index, model)

# Print the most relevant results (top_k closest documents)
print("Top results for query:", query)
for i, idx in enumerate(indices[0]):
    print(f"Result {i+1}:")
    if idx < len(embeddings_txt):
        source = 'Text (TXT)'
    elif idx < len(embeddings_txt) + len(embeddings_pdf):
        source = 'Text (PDF)'
    else:
        source = 'Text (Audio)'

    print(f"Source: {source} | Distance: {distances[0][i]}")
    # You can return the actual document or text associated with the embedding (you would need to store the documents in a list)
    print(f"Document Content: {all_embeddings[idx]} \n\n")

import faiss
import numpy as np

# Assume 'faiss_index' is your FAISS index that has been populated with document embeddings.
# 'document_map' is a dictionary that maps the index positions to your document contents (texts).
# 'query_embedding' is the embedding for the user query.

# Function to search FAISS index and retrieve the top k nearest documents
def retrieve_documents_from_query(query_embedding, k=3, index=index, model=model):  # Add model as argument
    """
    Retrieves documents from the FAISS index based on a query embedding.

    Args:
        query_embedding: The embedding of the query.
        k: The number of nearest documents to retrieve.
        index: The FAISS index.
        model: The model used to generate embeddings.

    Returns:
        A tuple containing the retrieved documents and their distances.
    """
    # Generate query embedding using the provided model
    query_embedding = model.encode([query_embedding])[0]  # Encode query and get the embedding

    # Convert query_embedding to a NumPy array with the correct data type and dimensions
    query_embedding = np.array([query_embedding]).astype(np.float32)

    # Get the dimensionality of the FAISS index
    index_dimension = index.d

    # Ensure the query embedding has the correct dimensionality
    if query_embedding.shape[1] != index_dimension:
        raise ValueError(f"Query embedding dimension ({query_embedding.shape[1]}) does not match index dimension ({index_dimension})")

    # Perform the search in the FAISS index
    distances, indices = index.search(query_embedding, k)  # Search top k results

    # Retrieve the corresponding documents using the indices
    retrieved_docs = [document_map[idx] for idx in indices[0]]

    return retrieved_docs, distances[0]


# Example query (use the actual query in your case)
query = "What is the process of material management in MES?"  # Use the query string directly

# Retrieve the top 3 most relevant documents based on the query
retrieved_documents, distances = retrieve_documents_from_query(query, k=3)  # Pass query to the function

# Print out the retrieved documents and their distances
for i, doc in enumerate(retrieved_documents):
    print(f"Result {i+1}:")
    print(f"Distance: {distances[i]}")
    print(f"Document Content: {doc}")
    print("\n")

"""Example Code for Summarizing Based on Embeddings:


"""

from transformers import T5ForConditionalGeneration, T5Tokenizer

# Function to summarize documents using a summarization model
def summarize_documents(documents, max_length=200):
    # Load pre-trained T5 model and tokenizer for summarization
    model_name = "t5-small"  # You can use a larger model like "t5-base" or "t5-large" for better results
    tokenizer = T5Tokenizer.from_pretrained(model_name)
    model = T5ForConditionalGeneration.from_pretrained(model_name)

    # Concatenate the top documents to create a single input text
    combined_text = " ".join(documents)  # Ensure you are concatenating full content here, not just titles

    # Tokenize the input text
    inputs = tokenizer.encode("summarize: " + combined_text, return_tensors="pt", max_length=512, truncation=True)

    # Generate summary
    summary_ids = model.generate(inputs, max_length=max_length, num_beams=4, early_stopping=True)

    # Decode and return the summary
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

# Example documents stored in your system (replace these with actual full-text content)
documents = [
    "Manufacturing Execution Systems (MES) are used to monitor and control manufacturing processes. An MES helps to optimize production scheduling, track inventory, and provide real-time updates on manufacturing operations.",
    "The Manufacturing Execution System is a critical component of modern manufacturing plants, facilitating the connection between raw material inputs and finished products through comprehensive data collection and process control.",
    "MES is a solution that supports businesses in streamlining production workflows, ensuring compliance with regulations, and improving quality control across manufacturing units."
]

# Summarize the documents
summary = summarize_documents(documents)
print("Generated Summary based on query embeddings:")
print(summary)

"""
**Using B5/BART transformer**"""

!pip install sentence-transformers transformers faiss-cpu

!pip install PyPDF2

!pip install SpeechRecognition

import numpy as np
import faiss

# Paths to pre-generated embeddings
embeddings_audio_path = "/content/drive/MyDrive/MES_Files/embeddings_audio.npy"
embeddings_pdf_path = "/content/drive/MyDrive/MES_Files/embeddings_pdf.npy"
embeddings_txt_path = "/content/drive/MyDrive/MES_Files/embeddings_txt.npy"

# Load embeddings
embeddings_audio = np.load(embeddings_audio_path).astype('float32')
embeddings_pdf = np.load(embeddings_pdf_path).astype('float32')
embeddings_txt = np.load(embeddings_txt_path).astype('float32')

# Combine embeddings into one array
embeddings = np.vstack([embeddings_audio, embeddings_pdf, embeddings_txt])

# Check the shape of the embeddings to ensure consistency
print(f"Embeddings shape: {embeddings.shape}")

# Initialize FAISS index
embedding_dim = embeddings.shape[1]  # Assuming all embeddings have the same dimension
faiss_index = faiss.IndexFlatL2(embedding_dim)  # Using L2 (Euclidean) distance
faiss_index.add(embeddings)

# Mapping embeddings to document paths
document_paths = [
    "/content/drive/MyDrive/MES_Files/embeddings_audio.npy",
    "/content/drive/MyDrive/MES_Files/embeddings_pdf.npy",
    "/content/drive/MyDrive/MES_Files/embeddings_txt.npy"
]

# Create a mapping from index to document path
doc_mapping = {i: path for i, path in enumerate(document_paths)}

print("FAISS index built and embeddings stored.")

from sentence_transformers import SentenceTransformer

# Load a pre-trained model for the query embedding
query_model = SentenceTransformer('all-MiniLM-L6-v2')

def get_relevant_documents(query, faiss_index, doc_mapping, top_k=3):
    """
    Retrieve the most relevant document paths for a query using FAISS.
    """
    # Convert the query to an embedding
    query_embedding = query_model.encode(query).astype('float32')

    # Perform the FAISS search
    distances, indices = faiss_index.search(query_embedding.reshape(1, -1), top_k)

    # Get the retrieved document paths
    retrieved_docs = [doc_mapping[idx] for idx in indices[0]]

    return retrieved_docs, distances[0]

# User query
query = "What is the process of material management in MES?"

# Retrieve relevant documents based on the query
retrieved_docs, distances = get_relevant_documents(query, faiss_index, doc_mapping)

print("Retrieved Document Paths:", retrieved_docs)
print("Distances:", distances)

from transformers import T5ForConditionalGeneration, T5Tokenizer

def read_file_content(file_path):
    """
    Dummy function to read content from various document formats.
    Replace with actual content reading code.
    """
    # For text, pdf, and audio (depending on your content type)
    # Here, simply simulate text reading
    try:
        if file_path.endswith('.txt'):
            with open(file_path, 'r') as file:
                return file.read()
        elif file_path.endswith('.pdf'):
            # Use a PDF reader here
            return "Sample PDF content about material management in MES."
        elif file_path.endswith('.mp4'):
            # Audio/video files processing (just for placeholder)
            return "Sample audio content about material management in MES."
        else:
            return "No content found."
    except Exception as e:
        return str(e)

def generate_response_from_documents_with_weighted_content(query, retrieved_docs, distances, model_name='t5-small'):
    """
    Generate a response using a transformer model based on the query and retrieved documents, considering their relevance (distances).
    """
    # Read content of retrieved documents
    retrieved_content = [read_file_content(doc) for doc in retrieved_docs]  # You can adjust the `read_file_content` to load real content if needed.

    # Print the retrieved content for debugging
    print("Retrieved Content:", retrieved_content)

    # Weight the content based on distance (inverse of distance: smaller distance = more weight)
    weighted_content = []
    for doc_content, dist in zip(retrieved_content, distances):
        weight = 1 / (dist + 1e-5)  # To avoid division by zero, we add a small value to the distance
        weighted_content.append((doc_content, weight))

    # Sort the documents by their weight (most relevant document first)
    weighted_content = sorted(weighted_content, key=lambda x: x[1], reverse=True)

    # Combine the weighted content
    combined_content = " ".join([doc[0] for doc in weighted_content])

    # Print the combined content for debugging
    print("Combined Content:", combined_content)

    # Load T5 model and tokenizer
    tokenizer = T5Tokenizer.from_pretrained(model_name)
    model = T5ForConditionalGeneration.from_pretrained(model_name)

    # Combine query and weighted content
    input_text = f"Query: {query} Context: {combined_content}"

    # Tokenize input
    inputs = tokenizer.encode(input_text, return_tensors='pt', max_length=512, truncation=True)

    # Generate response
    outputs = model.generate(inputs, max_length=150, num_beams=5, early_stopping=True)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    return response

# Example for testing
retrieved_docs = ["/content/drive/MyDrive/MES_text.txt", "/content/drive/MyDrive/MES_gmpua.pdf", "/content/drive/MyDrive/Manufacturing Execution System - What is it.mp4"]
distances = [0.94111496, 1.2923467, 1.2929051]  # Use the distances you obtained from FAISS

query = "What is process of material management in MES?"

# Generate response based on retrieved documents with weighted content
response = generate_response_from_documents_with_weighted_content(query, retrieved_docs, distances)

print("Generated Response:", response)

!pip install torch

import numpy as np
import faiss
from transformers import BertTokenizer, BertModel, pipeline
from sklearn.metrics.pairwise import cosine_similarity
import torch # Import the torch module


# Load pre-existing embeddings (assumed to be already generated and stored in FAISS)
embeddings_audio = np.load("/content/drive/MyDrive/MES_Files/embeddings_audio.npy")
embeddings_pdf = np.load("/content/drive/MyDrive/MES_Files/embeddings_pdf.npy")
embeddings_txt = np.load("/content/drive/MyDrive/MES_Files/embeddings_txt.npy")

# Combine embeddings from different formats into one vector database
embeddings = np.concatenate([embeddings_audio, embeddings_pdf, embeddings_txt], axis=0)

# FAISS index creation and loading
# Get the dimension of the embeddings
embedding_dim = embeddings.shape[1]
index = faiss.IndexFlatL2(embedding_dim)  # Initialize with the correct dimension
index.add(embeddings)

# User query (you can replace this with the dynamic input)
query = "What is the process of material management in MES?"

# Step 1: Convert query into embeddings using a transformer model
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased")

# Tokenize and get embeddings for the query
inputs = tokenizer(query, return_tensors="pt", padding=True, truncation=True, max_length=512)
with torch.no_grad():
    query_embedding = model(**inputs).last_hidden_state.mean(dim=1).cpu().numpy()

# Reshape or reduce the dimensionality of the query_embedding to match the FAISS index dimension
query_embedding = query_embedding[:, :embedding_dim] # Assuming you want to keep the first 384 dimensions


# Step 2: Perform cosine similarity search with FAISS index
distances, indices = index.search(query_embedding, k=3)  # Retrieve top 3 most relevant results

# Step 3: Retrieve and filter the content based on similarity scores
# Assuming you have corresponding documents or content at these indices
document_paths = ["/content/drive/MyDrive/MES_text.txt", "/content/drive/MyDrive/MES_gmpua.pdf", "/content/drive/MyDrive/Manufacturing Execution System - What is it.mp4"]

# Function to load content based on file type
def load_content(file_path):
    if file_path.endswith(".txt"):
        with open(file_path, "r") as f:
            return f.read()
    elif file_path.endswith(".pdf"):
        # Example: Use a PDF reading library like PyMuPDF or PyPDF2 to extract text from PDFs
        from PyPDF2 import PdfReader
        reader = PdfReader(file_path)
        return " ".join([page.extract_text() for page in reader.pages])
    elif file_path.endswith(".mp4"):
        # Use an audio-to-text model (like Speech-to-Text) or extract metadata
        return "Sample audio content about material management in MES."
    else:
        return ""

# Retrieve content for the most similar documents
retrieved_content = [load_content(document_paths[i]) for i in indices[0]]

# Combine content (filter based on relevance to material management)
combined_content = ""
for content in retrieved_content:
    if "material management" in content.lower():
        combined_content += content + "\n"

# Step 4: Generate response using the combined relevant content
# Here, we can use a BART model or any other summarization model to summarize the retrieved content.
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
summary = summarizer(combined_content, max_length=150, min_length=50, do_sample=False)

# Print the

# Print the final response
generated_response = f"Query: {query}\nContext: {combined_content}\nSummary: {summary[0]['summary_text']}"
print(generated_response)

"""Streamlit Deployment



---


"""

!pip install streamlit
!pip install faiss-cpu
!pip install transformers
!pip install PyPDF2
!pip install sklearn
!pip install speechrecognition

import numpy as np
import faiss
import torch
from transformers import BertTokenizer, BertModel, pipeline
from PyPDF2 import PdfReader
import speech_recognition as sr
import os
import streamlit as st

# Function to load content from files
def load_content(file_path):
    if file_path.endswith(".txt"):
        with open(file_path, "r") as f:
            return f.read()
    elif file_path.endswith(".pdf"):
        reader = PdfReader(file_path)
        return " ".join([page.extract_text() for page in reader.pages])
    elif file_path.endswith(".mp4"):
        return extract_audio_from_mp4(file_path)
    else:
        return ""

# Function to convert MP4 audio to text (simple placeholder)
def extract_audio_from_mp4(file_path):
    recognizer = sr.Recognizer()
    audio_file = file_path.replace('.mp4', '.wav')
    os.system(f"ffmpeg -i {file_path} {audio_file}")  # Convert MP4 to WAV
    with sr.AudioFile(audio_file) as source:
        audio = recognizer.record(source)
    try:
        text = recognizer.recognize_google(audio)
    except sr.UnknownValueError:
        text = "Audio not recognized."
    return text

# Function to load pre-existing embeddings and set up FAISS
def setup_faiss_index():
    embeddings_audio = np.load("/content/drive/MyDrive/MES_Files/embeddings_audio.npy")
    embeddings_pdf = np.load("/content/drive/MyDrive/MES_Files/embeddings_pdf.npy")
    embeddings_txt = np.load("/content/drive/MyDrive/MES_Files/embeddings_txt.npy")

    embeddings = np.concatenate([embeddings_audio, embeddings_pdf, embeddings_txt], axis=0)

    index = faiss.IndexFlatL2(embeddings.shape[1])  # Assuming 768-d embeddings
    index.add(embeddings)
    return index

# Function to generate the query embedding using BERT
def generate_query_embedding(query):
    tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
    model = BertModel.from_pretrained("bert-base-uncased")

    inputs = tokenizer(query, return_tensors="pt", padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        query_embedding = model(**inputs).last_hidden_state.mean(dim=1).cpu().numpy()

    return query_embedding

# Main Streamlit function
def main():
    st.title("Material Management in MES - Query Answering System")

    # User input (query)
    user_query = st.text_input("Enter your query:")

    if st.button("Get Answer"):
        if user_query:
            # Load FAISS index
            index = setup_faiss_index()

            # Step 1: Generate query embedding
            query_embedding = generate_query_embedding(user_query)

            # Step 2: Perform cosine similarity search
            distances, indices = index.search(query_embedding, k=3)

            # Retrieve document content from paths
            document_paths =["/content/drive/MyDrive/MES_text.txt", "/content/drive/MyDrive/MES_gmpua.pdf", "/content/drive/MyDrive/Manufacturing Execution System - What is it.mp4"]

            retrieved_content = [load_content(document_paths[i]) for i in indices[0]]

            # Filter content for "material management"
            combined_content = ""
            for content in retrieved_content:
                if "material management" in content.lower():
                    combined_content += content + "\n"

            # Step 3: Generate summary using BART model
            summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
            summary = summarizer(combined_content, max_length=150, min_length=50, do_sample=False)

            # Display final output
            generated_response = f"Query: {user_query}\nContext: {combined_content}\nSummary: {summary[0]['summary_text']}"
            st.write(generated_response)
        else:
            st.write("Please enter a query.")

# Run the Streamlit app
if __name__ == "__main__":
    main()

!pip install streamlit -q

